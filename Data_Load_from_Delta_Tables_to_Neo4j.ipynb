{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d22f737-d433-48f5-b17c-ae1d78516474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install neo4j tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f1be42-3284-4888-984d-d18ae9480a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98de920-28d6-41b8-8de7-eeb7fbacf99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:27:59,274 - INFO - Loading data from Databricks table: Beneficiary using Spark.\n2025-07-20 12:28:00,411 - INFO - Successfully loaded 63968 rows from Beneficiary.\n2025-07-20 12:28:00,412 - INFO - Loading data from Databricks table: Provider using Spark.\n2025-07-20 12:28:00,735 - INFO - Successfully loaded 1353 rows from Provider.\n2025-07-20 12:28:00,735 - INFO - Loading data from Databricks table: Inpatient_Claim using Spark.\n2025-07-20 12:28:01,138 - INFO - Successfully loaded 9551 rows from Inpatient_Claim.\n2025-07-20 12:28:01,140 - INFO - Loading data from Databricks table: Outpatient_Claim using Spark.\n2025-07-20 12:28:02,138 - INFO - Successfully loaded 125841 rows from Outpatient_Claim.\n2025-07-20 12:28:03,396 - INFO - Successfully connected to Neo4j using secrets.\n2025-07-20 12:28:03,396 - INFO - Creating Beneficiary nodes...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea8f325ee15480880baa3ce1ee8b6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beneficiary Nodes:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:28:40,745 - INFO - Created/Updated 63968 Beneficiary nodes.\n2025-07-20 12:28:40,745 - INFO - Creating Provider nodes...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5643654db2724dac921751005eb199e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Provider Nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:28:41,596 - INFO - Created/Updated 1353 Provider nodes.\n2025-07-20 12:28:41,597 - INFO - Creating Physician nodes from claim data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5d63175aea472588a4b1b5b2a0c944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Physician Nodes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:30:12,009 - INFO - Created/Updated 26591 Physician nodes.\n2025-07-20 12:30:12,012 - INFO - Creating Diagnosis nodes from claim data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6637e761126b433a9fa6b31b6bc23409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Diagnosis Nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:30:21,218 - INFO - Created/Updated 9057 Diagnosis nodes.\n2025-07-20 12:30:21,219 - INFO - Creating Procedure nodes from claim data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2e12d7e4f04115bfcf7bfe31376cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procedure Nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:30:22,045 - INFO - Created/Updated 809 Procedure nodes.\n2025-07-20 12:30:22,046 - INFO - Creating DiagnosisGroup nodes from claim data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584758ae2f4c45cbbd0e53603f20a79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DiagnosisGroup Nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:30:22,226 - INFO - Created/Updated 712 DiagnosisGroup nodes.\n2025-07-20 12:30:22,227 - INFO - Creating Inpatient_Claim nodes and relationships...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db31af415eff4363ab4675d578017189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inpatient_Claim Nodes & Relationships:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 12:33:14,328 - INFO - Created/Updated 9551 Inpatient_Claim nodes and their relationships.\n2025-07-20 12:33:14,332 - INFO - Creating Outpatient_Claim nodes and relationships...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4cad8b4e304cadb9d9801505e8603c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Outpatient_Claim Nodes & Relationships:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:48:26,437 - INFO - Created/Updated 125841 Outpatient_Claim nodes and their relationships.\n2025-07-20 13:48:26,438 - INFO - Creating Diagnosis -> DiagnosisGroup hierarchy relationships...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abe269f560241df81f29ede46c92d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DiagnosisGroup Hierarchy:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:48:43,747 - INFO - Created/Updated 6728 Diagnosis -> DiagnosisGroup relationships.\n2025-07-20 13:48:43,747 - INFO - Data loading to Neo4j completed successfully!\n2025-07-20 13:48:43,751 - INFO - Neo4j driver closed.\n"
     ]
    }
   ],
   "source": [
    "# Databricks to Neo4j Data Loading Notebook\n",
    "\n",
    "# This notebook demonstrates how to extract data from Databricks tables (without Spark Connectors)\n",
    "# and load it into a Neo4j graph database.\n",
    "# This version is optimized for running directly within a Databricks workspace.\n",
    "\n",
    "# --- 1. Prerequisites and Library Installation ---\n",
    "# Ensure you have the necessary libraries installed.\n",
    "# If running in a Databricks notebook, you might need to install these using pip.\n",
    "# %pip install neo4j pandas tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "from tqdm.notebook import tqdm # For progress bars in notebooks\n",
    "\n",
    "# SparkSession is implicitly available in Databricks notebooks as 'spark'\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"Neo4jDataLoad\").getOrCreate()\n",
    "\n",
    "# Configure logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "# IMPORTANT: Replace these placeholders with your actual secret scope name.\n",
    "# For production environments, consider using Databricks Secrets or environment variables\n",
    "# to manage sensitive credentials securely.\n",
    "\n",
    "# Neo4j Secret Scope Name\n",
    "SECRET_SCOPE_NAME = \"my-neo4j-scope\" # IMPORTANT: Replace with your Databricks secret scope name\n",
    "\n",
    "# Neo4j Connection Details (will be pulled from Databricks Secrets)\n",
    "# These variables will be populated using dbutils.secrets.get()\n",
    "NEO4J_URI = None\n",
    "NEO4J_USERNAME = None\n",
    "NEO4J_PASSWORD = None\n",
    "NEO4J_DATABASE = None\n",
    "\n",
    "# Batch size for Neo4j transactions (number of items per UNWIND statement)\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# --- 3. Databricks Data Extraction Function (using Spark directly) ---\n",
    "\n",
    "def load_data_from_databricks(table_name):\n",
    "    \"\"\"\n",
    "    Loads data from a specified Databricks table into a Pandas DataFrame\n",
    "    using the available SparkSession.\n",
    "    Args:\n",
    "        table_name (str): The name of the table to load.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the table data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data from Databricks table: {table_name} using Spark.\")\n",
    "    try:\n",
    "        # Access the global SparkSession directly available in Databricks notebooks\n",
    "        df_spark = spark.table(table_name)\n",
    "        df_pandas = df_spark.toPandas()\n",
    "        logging.info(f\"Successfully loaded {len(df_pandas)} rows from {table_name}.\")\n",
    "        return df_pandas\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from table {table_name} using Spark: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 4. Neo4j Connection and Utility Functions ---\n",
    "\n",
    "def get_neo4j_driver():\n",
    "    \"\"\"Establishes and returns a Neo4j GraphDatabase driver, pulling credentials from Databricks secrets.\"\"\"\n",
    "    global NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, NEO4J_DATABASE\n",
    "    try:\n",
    "        # Retrieve Neo4j credentials from Databricks secrets\n",
    "        # dbutils is a global object available in Databricks notebooks\n",
    "        NEO4J_URI = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-uri\")\n",
    "        NEO4J_USERNAME = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-username\")\n",
    "        NEO4J_PASSWORD = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-password\")\n",
    "        NEO4J_DATABASE = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-database\")\n",
    "\n",
    "        # For Neo4j Aura, the database name is typically part of the URI or defaults to 'neo4j'.\n",
    "        # The `database` parameter in GraphDatabase.driver might not be strictly necessary if it's 'neo4j'\n",
    "        # and included in the URI, but it's good practice to pass it if available.\n",
    "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD), database=NEO4J_DATABASE)\n",
    "        driver.verify_connectivity()\n",
    "        logging.info(\"Successfully connected to Neo4j using secrets.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error connecting to Neo4j. Ensure secrets are configured correctly: {e}\")\n",
    "        raise\n",
    "\n",
    "def execute_cypher_query(driver, query, parameters=None):\n",
    "    \"\"\"\n",
    "    Executes a Cypher query in a Neo4j session.\n",
    "    Args:\n",
    "        driver: The Neo4j GraphDatabase driver.\n",
    "        query (str): The Cypher query string.\n",
    "        parameters (dict, optional): Parameters for the Cypher query. Defaults to None.\n",
    "    Returns:\n",
    "        neo4j.Result: The result of the query execution.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        try:\n",
    "            result = session.run(query, parameters)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error executing Cypher query: {query}\\nParameters: {parameters}\\nError: {e}\")\n",
    "            raise\n",
    "\n",
    "def clear_neo4j_database(driver):\n",
    "    \"\"\"\n",
    "    Clears all nodes and relationships from the Neo4j database.\n",
    "    Use with extreme caution, especially in production environments!\n",
    "    \"\"\"\n",
    "    logging.warning(\"Clearing existing data in Neo4j. This cannot be undone!\")\n",
    "    query = \"MATCH (n) DETACH DELETE n\"\n",
    "    execute_cypher_query(driver, query)\n",
    "    logging.info(\"Neo4j database cleared.\")\n",
    "\n",
    "def prepare_properties(row):\n",
    "    \"\"\"\n",
    "    Prepares a dictionary of properties for a Neo4j node/relationship,\n",
    "    handling NaN values by converting them to None.\n",
    "    \"\"\"\n",
    "    props = row.where(pd.notna(row), None).to_dict()\n",
    "    # Convert any datetime objects to string for consistent Neo4j handling\n",
    "    for key, value in props.items():\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            props[key] = str(value.date())\n",
    "    return props\n",
    "\n",
    "# --- 5. Node and Relationship Creation Functions ---\n",
    "\n",
    "def create_beneficiary_nodes(driver, df_beneficiary):\n",
    "    \"\"\"Creates Beneficiary nodes in Neo4j.\"\"\"\n",
    "    logging.info(\"Creating Beneficiary nodes...\")\n",
    "    query = \"\"\"\n",
    "    UNWIND $data AS row\n",
    "    MERGE (b:Beneficiary {BeneID: row.BeneID})\n",
    "    SET b += row\n",
    "    \"\"\"\n",
    "    data = [prepare_properties(row) for _, row in df_beneficiary.iterrows()]\n",
    "    \n",
    "    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"Beneficiary Nodes\"):\n",
    "        batch = data[i:i + BATCH_SIZE]\n",
    "        execute_cypher_query(driver, query, parameters={\"data\": batch})\n",
    "    logging.info(f\"Created/Updated {len(df_beneficiary)} Beneficiary nodes.\")\n",
    "\n",
    "def create_provider_nodes(driver, df_provider):\n",
    "    \"\"\"Creates Provider nodes in Neo4j.\"\"\"\n",
    "    logging.info(\"Creating Provider nodes...\")\n",
    "    query = \"\"\"\n",
    "    UNWIND $data AS row\n",
    "    MERGE (p:Provider {ProviderID: row.ProviderID})\n",
    "    SET p += row\n",
    "    \"\"\"\n",
    "   \n",
    "    data = [prepare_properties(row) for _, row in df_provider.iterrows()]\n",
    "\n",
    "    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"Provider Nodes\"):\n",
    "        batch = data[i:i + BATCH_SIZE]\n",
    "        execute_cypher_query(driver, query, parameters={\"data\": batch})\n",
    "    logging.info(f\"Created/Updated {len(df_provider)} Provider nodes.\")\n",
    "\n",
    "def create_physician_nodes(driver, df_claims):\n",
    "    \"\"\"\n",
    "    Extracts unique physician IDs from claims data and creates Physician nodes.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating Physician nodes from claim data...\")\n",
    "    \n",
    "    # Concatenate physician columns and filter out NaN values.\n",
    "    # Assuming 'NA' string values are handled at the source.\n",
    "    all_physicians = pd.concat([\n",
    "        df_claims['AttendingPhysician'].dropna(),\n",
    "        df_claims['OperatingPhysician'].dropna(),\n",
    "        df_claims['OtherPhysician'].dropna()\n",
    "    ]).unique()\n",
    "\n",
    "    if len(all_physicians) == 0:\n",
    "        logging.info(\"No valid physician IDs found in claim data.\")\n",
    "        return\n",
    "\n",
    "    query = \"\"\"\n",
    "    UNWIND $physician_ids AS physician_id\n",
    "    MERGE (ph:Physician {Physician: physician_id})\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0, len(all_physicians), BATCH_SIZE), desc=\"Physician Nodes\"):\n",
    "        batch = all_physicians[i:i + BATCH_SIZE].tolist()\n",
    "        execute_cypher_query(driver, query, parameters={\"physician_ids\": batch})\n",
    "    logging.info(f\"Created/Updated {len(all_physicians)} Physician nodes.\")\n",
    "\n",
    "def create_diagnosis_nodes(driver, df_claims):\n",
    "    \"\"\"\n",
    "    Extracts unique diagnosis codes from claims data and creates Diagnosis nodes.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating Diagnosis nodes from claim data...\")\n",
    "    diagnosis_cols = [f'ClmDiagnosisCode_{i}' for i in range(1, 11)]\n",
    "    \n",
    "    all_diagnosis_codes = pd.Series(dtype='object')\n",
    "    for col in diagnosis_cols:\n",
    "        if col in df_claims.columns:\n",
    "            all_diagnosis_codes = pd.concat([all_diagnosis_codes, df_claims[col].dropna()])\n",
    "            \n",
    "    all_diagnosis_codes = all_diagnosis_codes.astype(str).str.strip().unique()\n",
    "    all_diagnosis_codes = all_diagnosis_codes[all_diagnosis_codes != ''] \n",
    "\n",
    "    if len(all_diagnosis_codes) == 0:\n",
    "        logging.info(\"No valid diagnosis codes found in claim data.\")\n",
    "        return\n",
    "\n",
    "    query = \"\"\"\n",
    "    UNWIND $diagnosis_codes AS diagnosis_code\n",
    "    MERGE (d:Diagnosis {DiagnosisCode: diagnosis_code})\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0, len(all_diagnosis_codes), BATCH_SIZE), desc=\"Diagnosis Nodes\"):\n",
    "        batch = all_diagnosis_codes[i:i + BATCH_SIZE].tolist()\n",
    "        execute_cypher_query(driver, query, parameters={\"diagnosis_codes\": batch})\n",
    "    logging.info(f\"Created/Updated {len(all_diagnosis_codes)} Diagnosis nodes.\")\n",
    "\n",
    "def create_procedure_nodes(driver, df_claims):\n",
    "    \"\"\"\n",
    "    Extracts unique procedure codes from claims data and creates Procedure nodes.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating Procedure nodes from claim data...\")\n",
    "    procedure_cols = [f'ClmProcedureCode_{i}' for i in range(1, 7)]\n",
    "    \n",
    "    all_procedure_codes = pd.Series(dtype='object')\n",
    "    for col in procedure_cols:\n",
    "        if col in df_claims.columns:\n",
    "            # Convert to string before dropping NA to handle potential numeric/mixed types\n",
    "            all_procedure_codes = pd.concat([all_procedure_codes, df_claims[col].astype(str).dropna()])\n",
    "            \n",
    "    all_procedure_codes = all_procedure_codes.astype(str).str.strip().unique()\n",
    "    all_procedure_codes = all_procedure_codes[all_procedure_codes != ''] \n",
    "\n",
    "    if len(all_procedure_codes) == 0:\n",
    "        logging.info(\"No valid procedure codes found in claim data.\")\n",
    "        return\n",
    "\n",
    "    query = \"\"\"\n",
    "    UNWIND $procedure_codes AS procedure_code\n",
    "    MERGE (proc:Procedure {ProcedureCode: procedure_code})\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0, len(all_procedure_codes), BATCH_SIZE), desc=\"Procedure Nodes\"):\n",
    "        batch = all_procedure_codes[i:i + BATCH_SIZE].tolist()\n",
    "        execute_cypher_query(driver, query, parameters={\"procedure_codes\": batch})\n",
    "    logging.info(f\"Created/Updated {len(all_procedure_codes)} Procedure nodes.\")\n",
    "\n",
    "def create_diagnosis_group_nodes(driver, df_claims):\n",
    "    \"\"\"\n",
    "    Extracts unique DiagnosisGroupCode from claims data and creates DiagnosisGroup nodes.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating DiagnosisGroup nodes from claim data...\")\n",
    "    \n",
    "    all_diagnosis_group_codes = df_claims['DiagnosisGroupCode'].dropna().astype(str).str.strip().unique()\n",
    "    all_diagnosis_group_codes = all_diagnosis_group_codes[all_diagnosis_group_codes != '']\n",
    "\n",
    "    if len(all_diagnosis_group_codes) == 0:\n",
    "        logging.info(\"No valid diagnosis group codes found in claim data.\")\n",
    "        return\n",
    "\n",
    "    query = \"\"\"\n",
    "    UNWIND $group_codes AS group_code\n",
    "    MERGE (dg:DiagnosisGroup {DiagnosisGroupCode: group_code})\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0, len(all_diagnosis_group_codes), BATCH_SIZE), desc=\"DiagnosisGroup Nodes\"):\n",
    "        batch = all_diagnosis_group_codes[i:i + BATCH_SIZE].tolist()\n",
    "        execute_cypher_query(driver, query, parameters={\"group_codes\": batch})\n",
    "    logging.info(f\"Created/Updated {len(all_diagnosis_group_codes)} DiagnosisGroup nodes.\")\n",
    "\n",
    "def create_diagnosis_group_hierarchy(driver, df_claims):\n",
    "    \"\"\"\n",
    "    Creates IS_PART_OF_GROUP relationships between Diagnosis and DiagnosisGroup nodes.\n",
    "    Links ClmAdmitDiagnosisCode to its DiagnosisGroupCode.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating Diagnosis -> DiagnosisGroup hierarchy relationships...\")\n",
    "    \n",
    "    # Filter claims to only include those with valid ClmAdmitDiagnosisCode and DiagnosisGroupCode\n",
    "    df_filtered = df_claims[\n",
    "        df_claims['ClmAdmitDiagnosisCode'].notna() & \n",
    "        (df_claims['ClmAdmitDiagnosisCode'] != '') &\n",
    "        df_claims['DiagnosisGroupCode'].notna() &\n",
    "        (df_claims['DiagnosisGroupCode'] != '')\n",
    "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Ensure these columns are string type for consistent merging\n",
    "    df_filtered['ClmAdmitDiagnosisCode'] = df_filtered['ClmAdmitDiagnosisCode'].astype(str).str.strip()\n",
    "    df_filtered['DiagnosisGroupCode'] = df_filtered['DiagnosisGroupCode'].astype(str).str.strip()\n",
    "\n",
    "    # Get unique pairs to avoid redundant MERGE operations\n",
    "    unique_diagnosis_group_pairs = df_filtered[['ClmAdmitDiagnosisCode', 'DiagnosisGroupCode']].drop_duplicates().to_dict(orient='records')\n",
    "\n",
    "    if not unique_diagnosis_group_pairs:\n",
    "        logging.info(\"No valid diagnosis-group pairs found for hierarchy creation.\")\n",
    "        return\n",
    "\n",
    "    query = \"\"\"\n",
    "    UNWIND $data AS pair\n",
    "    MATCH (d:Diagnosis {DiagnosisCode: pair.ClmAdmitDiagnosisCode})\n",
    "    MATCH (dg:DiagnosisGroup {DiagnosisGroupCode: pair.DiagnosisGroupCode})\n",
    "    MERGE (d)-[:IS_PART_OF_GROUP]->(dg)\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0, len(unique_diagnosis_group_pairs), BATCH_SIZE), desc=\"DiagnosisGroup Hierarchy\"):\n",
    "        batch = unique_diagnosis_group_pairs[i:i + BATCH_SIZE]\n",
    "        execute_cypher_query(driver, query, parameters={\"data\": batch})\n",
    "    logging.info(f\"Created/Updated {len(unique_diagnosis_group_pairs)} Diagnosis -> DiagnosisGroup relationships.\")\n",
    "\n",
    "\n",
    "def create_claim_nodes_and_relationships(driver, df_claims, claim_type):\n",
    "    \"\"\"\n",
    "    Creates Claim nodes (Inpatient_Claim or Outpatient_Claim) and their relationships\n",
    "    to Beneficiary, Provider, Physician, Diagnosis, and Procedure nodes.\n",
    "    Args:\n",
    "        driver: The Neo4j GraphDatabase driver.\n",
    "        df_claims (pd.DataFrame): DataFrame containing claim data.\n",
    "        claim_type (str): 'Inpatient_Claim' or 'Outpatient_Claim'.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Creating {claim_type} nodes and relationships...\")\n",
    "    \n",
    "    # Ensure date columns are handled as strings for Neo4j properties\n",
    "    date_cols = ['ClaimStartDt', 'ClaimEndDt', 'AdmissionDt', 'DischargeDt']\n",
    "    for col in date_cols:\n",
    "        if col in df_claims.columns:\n",
    "            df_claims[col] = df_claims[col].dt.strftime('%Y-%m-%d').fillna('')\n",
    "\n",
    "    data = [prepare_properties(row) for _, row in df_claims.iterrows()]\n",
    "\n",
    "    # Cypher query for creating Claim nodes and relationships\n",
    "    query = f\"\"\"\n",
    "    UNWIND $data AS row\n",
    "    MERGE (c:{claim_type} {{ClaimID: row.ClaimID}})\n",
    "    SET c += row\n",
    "    \n",
    "    // Relationship to Beneficiary\n",
    "    MERGE (b:Beneficiary {{BeneID: row.BeneID}})\n",
    "    MERGE (c)-[:CLAIM_BENEFICIARY]->(b)\n",
    "    \n",
    "    // Relationship to Provider\n",
    "    MERGE (p:Provider {{ProviderID: row.ProviderID}})\n",
    "    MERGE (c)-[:HAS_{claim_type.upper()}]->(p)\n",
    "    \n",
    "    // Relationship to Attending Physician\n",
    "    WITH c, row\n",
    "    WHERE row.AttendingPhysician IS NOT NULL AND row.AttendingPhysician <> ''\n",
    "    MERGE (att_ph:Physician {{Physician: row.AttendingPhysician}})\n",
    "    MERGE (c)-[:ATTENDED_BY]->(att_ph)\n",
    "    \n",
    "    // Relationship to Operating Physician\n",
    "    WITH c, row\n",
    "    WHERE row.OperatingPhysician IS NOT NULL AND row.OperatingPhysician <> ''\n",
    "    MERGE (op_ph:Physician {{Physician: row.OperatingPhysician}})\n",
    "    MERGE (c)-[:OPERATED_BY]->(op_ph)\n",
    "\n",
    "    // Relationship to Other Physician\n",
    "    WITH c, row\n",
    "    WHERE row.OtherPhysician IS NOT NULL AND row.OtherPhysician <> ''\n",
    "    MERGE (other_ph:Physician {{Physician: row.OtherPhysician}})\n",
    "    MERGE (c)-[:ATTENDED_BY]->(other_ph)\n",
    "\n",
    "    // Relationships to Diagnosis Nodes (ClmDiagnosisCode_1 to ClmDiagnosisCode_10)\n",
    "    WITH c, row\n",
    "    UNWIND range(1, 10) AS i\n",
    "    WITH c, row, 'ClmDiagnosisCode_' + toString(i) AS diag_col_name\n",
    "    WHERE row[diag_col_name] IS NOT NULL AND row[diag_col_name] <> ''\n",
    "    MERGE (diag:Diagnosis {{DiagnosisCode: row[diag_col_name]}})\n",
    "    MERGE (c)-[:HAS_DIAGNOSIS]->(diag)\n",
    "\n",
    "    // Relationships to Procedure Nodes (ClmProcedureCode_1 to ClmProcedureCode_6)\n",
    "    WITH c, row\n",
    "    UNWIND range(1, 6) AS i\n",
    "    WITH c, row, 'ClmProcedureCode_' + toString(i) AS proc_col_name\n",
    "    WHERE row[proc_col_name] IS NOT NULL AND row[proc_col_name] <> ''\n",
    "    MERGE (proc:Procedure {{ProcedureCode: toString(row[proc_col_name])}}) \n",
    "    MERGE (c)-[:HAS_PROCEDURE]->(proc)\n",
    "    \"\"\"\n",
    "\n",
    "    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=f\"{claim_type} Nodes & Relationships\"):\n",
    "        batch = data[i:i + BATCH_SIZE]\n",
    "        execute_cypher_query(driver, query, parameters={\"data\": batch})\n",
    "    logging.info(f\"Created/Updated {len(df_claims)} {claim_type} nodes and their relationships.\")\n",
    "\n",
    "# --- 6. Main Execution Flow ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data loading process.\"\"\"\n",
    "    neo4j_driver = None\n",
    "    try:\n",
    "        # 1. Load Data from Databricks Tables using Spark directly\n",
    "        df_beneficiary = load_data_from_databricks(\"Beneficiary\")\n",
    "        df_provider = load_data_from_databricks(\"Provider\")\n",
    "        df_inpatient_claim = load_data_from_databricks(\"Inpatient_Claim\")\n",
    "        df_outpatient_claim = load_data_from_databricks(\"Outpatient_Claim\")\n",
    "\n",
    "        # Concatenate claims for physician, diagnosis, procedure, and diagnosis group node creation\n",
    "        df_all_claims = pd.concat([df_inpatient_claim, df_outpatient_claim], ignore_index=True)\n",
    "\n",
    "        # Basic check if data was loaded and apply filtering for primary keys\n",
    "        # Assuming 'NA' string values are handled at the source,\n",
    "        # so dropping NaN values should be sufficient for primary keys.\n",
    "        if df_beneficiary is not None:\n",
    "            initial_rows = len(df_beneficiary)\n",
    "            df_beneficiary.dropna(subset=['BeneID'], inplace=True)\n",
    "            if len(df_beneficiary) < initial_rows:\n",
    "                logging.warning(f\"Filtered out {initial_rows - len(df_beneficiary)} rows from Beneficiary due to missing BeneID.\")\n",
    "        else:\n",
    "            logging.error(\"Beneficiary DataFrame failed to load.\")\n",
    "            return\n",
    "\n",
    "        if df_provider is not None:\n",
    "            initial_rows = len(df_provider)\n",
    "            # Now using 'ProviderID' as the column name directly from the source table\n",
    "            df_provider.dropna(subset=['ProviderID'], inplace=True)\n",
    "            if len(df_provider) < initial_rows:\n",
    "                logging.warning(f\"Filtered out {initial_rows - len(df_provider)} rows from Provider due to missing Provider ID.\")\n",
    "        else:\n",
    "            logging.error(\"Provider DataFrame failed to load.\")\n",
    "            return\n",
    "\n",
    "        if df_inpatient_claim is not None:\n",
    "            initial_rows = len(df_inpatient_claim)\n",
    "            df_inpatient_claim.dropna(subset=['ClaimID'], inplace=True)\n",
    "            if len(df_inpatient_claim) < initial_rows:\n",
    "                logging.warning(f\"Filtered out {initial_rows - len(df_inpatient_claim)} rows from Inpatient_Claim due to missing ClaimID.\")\n",
    "        else:\n",
    "            logging.error(\"Inpatient_Claim DataFrame failed to load.\")\n",
    "            return\n",
    "\n",
    "        if df_outpatient_claim is not None:\n",
    "            initial_rows = len(df_outpatient_claim)\n",
    "            df_outpatient_claim.dropna(subset=['ClaimID'], inplace=True)\n",
    "            if len(df_outpatient_claim) < initial_rows:\n",
    "                logging.warning(f\"Filtered out {initial_rows - len(df_outpatient_claim)} rows from Outpatient_Claim due to missing ClaimID.\")\n",
    "        else:\n",
    "            logging.error(\"Outpatient_Claim DataFrame failed to load.\")\n",
    "            return\n",
    "\n",
    "        # Convert date columns to datetime objects for consistent handling before passing to prepare_properties\n",
    "        # This helps ensure dates are formatted correctly when converted to strings for Neo4j.\n",
    "        for df in [df_inpatient_claim, df_outpatient_claim]:\n",
    "            for col in ['ClaimStartDt', 'ClaimEndDt', 'AdmissionDt', 'DischargeDt']:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "        # 2. Connect to Neo4j (credentials now pulled from secrets)\n",
    "        neo4j_driver = get_neo4j_driver()\n",
    "\n",
    "        # Optional: Clear existing data in Neo4j before import\n",
    "        # Uncomment the line below if you want to clear the database on each run.\n",
    "        # clear_neo4j_database(neo4j_driver)\n",
    "\n",
    "        # 3. Create Nodes and Relationships in Neo4j\n",
    "        create_beneficiary_nodes(neo4j_driver, df_beneficiary)\n",
    "        create_provider_nodes(neo4j_driver, df_provider)\n",
    "        \n",
    "        # Create Physician, Diagnosis, Procedure, and DiagnosisGroup nodes from combined claims data\n",
    "        create_physician_nodes(neo4j_driver, df_all_claims)\n",
    "        create_diagnosis_nodes(neo4j_driver, df_all_claims)\n",
    "        create_procedure_nodes(neo4j_driver, df_all_claims)\n",
    "        create_diagnosis_group_nodes(neo4j_driver, df_all_claims)\n",
    "\n",
    "        # Create Claim nodes and their respective relationships\n",
    "        create_claim_nodes_and_relationships(neo4j_driver, df_inpatient_claim, \"Inpatient_Claim\")\n",
    "        create_claim_nodes_and_relationships(neo4j_driver, df_outpatient_claim, \"Outpatient_Claim\")\n",
    "\n",
    "        # Create hierarchy relationship for DiagnosisGroup\n",
    "        create_diagnosis_group_hierarchy(neo4j_driver, df_all_claims)\n",
    "\n",
    "        logging.info(\"Data loading to Neo4j completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"An unhandled error occurred during the process: {e}\")\n",
    "    finally:\n",
    "        # Close connections\n",
    "        if neo4j_driver:\n",
    "            neo4j_driver.close()\n",
    "            logging.info(\"Neo4j driver closed.\")\n",
    "\n",
    "# --- Execute the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# --- 7. Verification (Optional) ---\n",
    "# After running the notebook, you can connect to your Neo4j Browser (usually at http://localhost:7474)\n",
    "# and run some Cypher queries to verify the data.\n",
    "\n",
    "# Example Verification Queries:\n",
    "# - Count all nodes: MATCH (n) RETURN count(n)\n",
    "# - Count nodes by label: MATCH (n:Beneficiary) RETURN count(n)\n",
    "# - Find a beneficiary and their claims:\n",
    "#   MATCH (b:Beneficiary {BeneID: 'BENEID_EXAMPLE'})-[:CLAIM_BENEFICIARY]-(c)\n",
    "#   RETURN b, c\n",
    "# - Find a provider and their claims:\n",
    "#   MATCH (p:Provider {ProviderID: 'PROVIDERID_EXAMPLE'})-[:HAS_INPATIENT_CLAIM|HAS_OUTPATIENT_CLAIM]-(c)\n",
    "#   RETURN p, c\n",
    "# - Find a physician and claims they attended:\n",
    "#   MATCH (ph:Physician {Physician: 'PHYSICIAN_EXAMPLE'})<-[:ATTENDED_BY|OPERATED_BY]-(c)\n",
    "#   RETURN ph, c\n",
    "# - Find a diagnosis and its group:\n",
    "#   MATCH (d:Diagnosis {DiagnosisCode: 'DIAGNOSIS_CODE_EXAMPLE'})-[:IS_PART_OF_GROUP]->(dg:DiagnosisGroup)\n",
    "#   RETURN d, dg\n",
    "# - Find a claim and its diagnoses:\n",
    "#   MATCH (c:Inpatient_Claim {ClaimID: 'CLAIMID_EXAMPLE'})-[:HAS_DIAGNOSIS]->(d:Diagnosis)\n",
    "#   RETURN c, d\n",
    "# - Find a claim and its procedures:\n",
    "#   MATCH (c:Outpatient_Claim {ClaimID: 'CLAIMID_EXAMPLE'})-[:HAS_PROCEDURE]->(p:Procedure)\n",
    "#   RETURN c, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfabbb8-9b39-4a84-bc52-c4075a52a581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Load_from_Delta_Tables_to_Neo4j",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}