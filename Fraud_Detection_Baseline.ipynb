{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd650cf-169a-408c-818a-39503e5f2b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Fraud_Detection_With_Graph_Embeddings.py\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Fraud Detection with Neo4j Graph Embeddings (Serverless-Compatible)\n",
    "# MAGIC This notebook connects to a Neo4j Aura instance, runs graph algorithms via GDS, and extracts provider-level embeddings for fraud prediction.\n",
    "\n",
    "# COMMAND ----------\n",
    "# 1. Setup SparkSession and Retrieve Secrets for Neo4j GDS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from graphdatascience.session import DbmsConnectionInfo, GdsSessions, AuraAPICredentials, SessionMemory, CloudLocation\n",
    "import os\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "SECRET_SCOPE_NAME = \"my-neo4j-scope\"\n",
    "GDS_SESSION_NAME = \"gs-gds-session\"\n",
    "GDS_SESSION_MEMORY = SessionMemory.m_4GB\n",
    "GDS_CLOUD_PROVIDER = \"gcp\"\n",
    "GDS_CLOUD_REGION = \"us-east-1\"\n",
    "\n",
    "print(\"--- Cell 1: SparkSession Initialization and Credential Retrieval ---\")\n",
    "\n",
    "try:\n",
    "    neo4j_url = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-url\")\n",
    "    neo4j_username = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-username\")\n",
    "    neo4j_password = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-password\")\n",
    "    neo4j_dbname = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"neo4j-database\")\n",
    "\n",
    "    aura_client_id = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"aura-gds-client-id\")\n",
    "    aura_client_secret = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"aura-gds-client-secret\")\n",
    "    aura_project_id = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=\"aura-gds-project-id\")\n",
    "\n",
    "    print(\"Successfully retrieved all credentials from Databricks Secrets.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to retrieve secrets. Please ensure your secret scope '{SECRET_SCOPE_NAME}' exists and contains all required keys. Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Neo4jGDSIntegration\")\n",
    "    .config(\"neo4j.url\", neo4j_url)\n",
    "    .config(\"neo4j.authentication.basic.username\", neo4j_username)\n",
    "    .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "    .config(\"neo4j.database\", neo4j_dbname)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession initialized with Neo4j configurations.\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Neo4jGDSIntegration\")\n",
    "    .config(\"neo4j.url\", neo4j_url)\n",
    "    .config(\"neo4j.authentication.basic.username\", neo4j_username)\n",
    "    .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "    .config(\"neo4j.database\", neo4j_dbname)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession initialized with Neo4j configuration.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 2. Initialize GDS Session\n",
    "\n",
    "print(\"--- GDS Session Initialization ---\")\n",
    "\n",
    "# Initialize AuraAPICredentials with retrieved secrets\n",
    "api_credentials = AuraAPICredentials(\n",
    "    client_id=aura_client_id,\n",
    "    client_secret=aura_client_secret,\n",
    "    project_id=aura_project_id\n",
    ")\n",
    "\n",
    "sessions = GdsSessions(api_credentials=api_credentials)\n",
    "\n",
    "# Create or Get GDS Session\n",
    "try:\n",
    "    gds = sessions.get_or_create(\n",
    "        session_name=GDS_SESSION_NAME,\n",
    "        memory=GDS_SESSION_MEMORY,\n",
    "        db_connection=DbmsConnectionInfo(neo4j_url, neo4j_username, neo4j_password),\n",
    "    )\n",
    "    print(f\"Successfully connected to GDS session: '{GDS_SESSION_NAME}'\")\n",
    "    print(f\"GDS Version: {gds.version()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to create or connect to GDS session '{GDS_SESSION_NAME}'. Details: {e}\")\n",
    "    raise\n",
    "print(\"GDS session is active.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 5. Train Fraud Model with Augmented Features\n",
    "\n",
    "print(\"ðŸ“¦ Projecting graph using Cypher query and running fastRP...\")\n",
    "\n",
    "# Drop graph if it already exists to avoid ALREADY_EXISTS error\n",
    "if gds.graph.exists(\"provider_graph\")[\"exists\"]:\n",
    "    gds.graph.drop(\"provider_graph\")\n",
    "\n",
    "G, result = gds.graph.project(\n",
    "    graph_name=\"provider_graph\",\n",
    "    query=\"\"\"\n",
    "    CALL {\n",
    "        MATCH (p:Provider)\n",
    "        OPTIONAL MATCH (p)<-[r1:HAS_INPATIENT_CLAIM|HAS_OUTPATIENT_CLAIM]-(claim)\n",
    "        RETURN p AS source, r1 AS rel, claim AS target, {} AS sourceNodeProperties, {} AS targetNodeProperties\n",
    "    }\n",
    "    RETURN gds.graph.project.remote(source, target, {\n",
    "      sourceNodeProperties: sourceNodeProperties,\n",
    "      targetNodeProperties: targetNodeProperties,\n",
    "      sourceNodeLabels: labels(source),\n",
    "      targetNodeLabels: labels(target),\n",
    "      relationshipType: type(rel),\n",
    "      relationshipProperties: properties(rel)\n",
    "    })\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "print(f\"Projected graph '{G.name}' with {G.node_count()} nodes.\")\n",
    "\n",
    "# Run fastRP\n",
    "embedding_result = gds.fastRP.mutate(\n",
    "    G,\n",
    "    embeddingDimension=64,\n",
    "    mutateProperty=\"embedding\",\n",
    "    iterationWeights=[0.8, 1, 1, 0.5]\n",
    ")\n",
    "print(\"fastRP embeddings generated.\")\n",
    "\n",
    "# Store embeddings in memory for export\n",
    "embedding_df = gds.graph.nodeProperties.stream(G, \"embedding\")\n",
    "print(\"Embedding properties streamed.\")\n",
    "\n",
    "# Convert to Pandas and then Spark DataFrame\n",
    "import pandas as pd\n",
    "pdf_embeddings = pd.DataFrame(embedding_df)\n",
    "print(\"Embedding DataFrame preview:\")\n",
    "print(pdf_embeddings.head())\n",
    "\n",
    "# Confirm available columns\n",
    "print(\"Available columns:\", pdf_embeddings.columns.tolist())\n",
    "embedding_col_name = 'embedding' if 'embedding' in pdf_embeddings.columns else 'nodeProperty'\n",
    "def safe_float_embedding(val):\n",
    "    if isinstance(val, (list, tuple)) and all(isinstance(e, (int, float)) for e in val):\n",
    "        return [float(e) for e in val]\n",
    "    return []  # return empty embedding if malformed\n",
    "\n",
    "pdf_embeddings[embedding_col_name] = pdf_embeddings[embedding_col_name].apply(safe_float_embedding)\n",
    "pdf_embeddings.rename(columns={\"nodeId\": \"ProviderID\", embedding_col_name: \"embedding\"}, inplace=True)\n",
    "pdf_embeddings[\"ProviderID\"] = pdf_embeddings[\"ProviderID\"].astype(str)\n",
    "pdf_embeddings[\"ProviderID\"] = pdf_embeddings[\"ProviderID\"].str.replace(\"PRV\", \"\", regex=False)\n",
    "pdf_embeddings.rename(columns={\"nodeId\": \"ProviderID\"}, inplace=True)\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ProviderID\", StringType(), True),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "spark_embeddings_df = spark.createDataFrame(pdf_embeddings, schema)\n",
    "print(\"Embedding DataFrame created in Spark.\")\n",
    "\n",
    "# Load baseline features\n",
    "baseline_df = spark.read.table(\"workspace.default.temp_provider_features_for_augmented_run\")\n",
    "\n",
    "# Join on ProviderID\n",
    "augmented_df = baseline_df.join(spark_embeddings_df, on=\"ProviderID\", how=\"inner\")\n",
    "print(f\"Joined DataFrame has {augmented_df.count()} rows and {len(augmented_df.columns)} columns.\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for sklearn training\n",
    "augmented_pdf = augmented_df.toPandas()\n",
    "\n",
    "X_aug = augmented_pdf.drop(columns=[\"ProviderID\", \"PotentialFraud\"])\n",
    "y_aug = augmented_pdf[\"PotentialFraud\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_aug, y_aug, stratify=y_aug, random_state=42)\n",
    "\n",
    "model_aug = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_aug.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_aug.predict(X_test)\n",
    "y_proba = model_aug.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision = precis\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fraud_Detection_Baseline.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
