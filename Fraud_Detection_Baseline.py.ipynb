{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd650cf-169a-408c-818a-39503e5f2b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Fraud_Detection_Baseline_Serverless.py\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Fraud Detection Baseline (Serverless-Compatible)\n",
    "# MAGIC This notebook builds a fraud detection baseline using Pandas + scikit-learn, suitable for Databricks Serverless.\n",
    "\n",
    "# COMMAND ----------\n",
    "# 1. Setup and Imports\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr, col, regexp_replace, to_date, year, current_date, avg\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ProviderFraudDetectionServerless\").getOrCreate()\n",
    "\n",
    "# Set MLflow experiment and registry\n",
    "MLFLOW_EXPERIMENT_NAME = \"/Shared/Provider_Fraud_Detection_Graph_Embeddings\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "LABEL_COL = \"PotentialFraud\"\n",
    "\n",
    "print(f\"MLflow Experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "print(f\"MLflow Registry URI: {mlflow.get_registry_uri()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 2. Load Data\n",
    "\n",
    "print(\"Loading Databricks tables...\")\n",
    "df_providers_train = spark.read.table(\"workspace.default.provider_train\")\n",
    "df_beneficiary_train = spark.read.table(\"workspace.default.beneficiary_train\")\n",
    "df_inpatient_train = spark.read.table(\"workspace.default.inpatient_claim_train\")\n",
    "df_outpatient_train = spark.read.table(\"workspace.default.outpatient_claim_train\")\n",
    "print(\"Tables loaded.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 3. Clean and Parse DOB\n",
    "\n",
    "print(\"Cleaning DOB column...\")\n",
    "# Attempt to convert MM/dd/yy to yyyy-MM-dd by inferring 20th or 21st century\n",
    "# This assumes any 2-digit year >= 30 is from 1900s, otherwise 2000s\n",
    "century_udf = F.udf(lambda x: f\"19{x}\" if int(x) >= 30 else f\"20{x}\")\n",
    "df_beneficiary_train = df_beneficiary_train.withColumn(\n",
    "    \"DOB_fixed\",\n",
    "    F.when(F.col(\"DOB\").rlike(\"^\\\\d{1,2}/\\\\d{1,2}/\\\\d{2}$\"),\n",
    "           F.concat_ws(\"-\",\n",
    "               century_udf(F.regexp_extract(\"DOB\", r\"\\\\d{1,2}/\\\\d{1,2}/(\\\\d{2})\", 1)),\n",
    "               F.lpad(F.regexp_extract(\"DOB\", r\"(\\\\d{1,2})/\\\\d{1,2}/\\\\d{2}\", 1), 2, '0'),\n",
    "               F.lpad(F.regexp_extract(\"DOB\", r\"\\\\d{1,2}/(\\\\d{1,2})/\\\\d{2}\", 1), 2, '0')\n",
    "           )\n",
    "       ).otherwise(F.col(\"DOB\"))\n",
    ")\n",
    "\n",
    "# Parse to date\n",
    "df_beneficiary_train = df_beneficiary_train.withColumn(\n",
    "    \"DOB_clean\",\n",
    "    to_date(\"DOB_fixed\", \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Filter invalid or unrealistic DOBs\n",
    "print(\"Filtering invalid DOBs...\")\n",
    "df_beneficiary_train = df_beneficiary_train.filter(\n",
    "    (col(\"DOB_clean\").isNotNull()) &\n",
    "    (year(\"DOB_clean\") <= year(current_date()) - 18) &\n",
    "    (year(\"DOB_clean\") >= 1900)\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 4. Feature Engineering\n",
    "\n",
    "print(\"Aggregating inpatient claims...\")\n",
    "df_inpatient_agg = df_inpatient_train.groupBy(\"ProviderID\").agg(\n",
    "    F.count(\"ClaimID\").alias(\"Inpatient_ClaimCount\"),\n",
    "    F.sum(\"InscClaimAmtReimbursed\").alias(\"Inpatient_TotalClaimAmt\"),\n",
    "    F.avg(\"InscClaimAmtReimbursed\").alias(\"Inpatient_AvgClaimAmt\"),\n",
    "    F.countDistinct(\"BeneID\").alias(\"Inpatient_UniquePatients_Claim\"),\n",
    "    F.countDistinct(\"AttendingPhysician\").alias(\"Inpatient_UniqueAttendingPhysicians\"),\n",
    "    F.countDistinct(\"OperatingPhysician\").alias(\"Inpatient_UniqueOperatingPhysicians\"),\n",
    "    F.countDistinct(\"OtherPhysician\").alias(\"Inpatient_UniqueOtherPhysicians\"),\n",
    "    F.avg(F.datediff(\"DischargeDt\", \"AdmissionDt\")).alias(\"Inpatient_AvgAdmissionDuration\"),\n",
    "    F.sum(\"DeductibleAmtPaid\").alias(\"Inpatient_TotalDeductiblePaid\")\n",
    ")\n",
    "\n",
    "print(\"Aggregating outpatient claims...\")\n",
    "df_outpatient_agg = df_outpatient_train.groupBy(\"ProviderID\").agg(\n",
    "    F.count(\"ClaimID\").alias(\"Outpatient_ClaimCount\"),\n",
    "    F.sum(\"InscClaimAmtReimbursed\").alias(\"Outpatient_TotalClaimAmt\"),\n",
    "    F.avg(\"InscClaimAmtReimbursed\").alias(\"Outpatient_AvgClaimAmt\"),\n",
    "    F.countDistinct(\"BeneID\").alias(\"Outpatient_UniquePatients_Claim\"),\n",
    "    F.countDistinct(\"AttendingPhysician\").alias(\"Outpatient_UniqueAttendingPhysicians\"),\n",
    "    F.countDistinct(\"OperatingPhysician\").alias(\"Outpatient_UniqueOperatingPhysicians\"),\n",
    "    F.countDistinct(\"OtherPhysician\").alias(\"Outpatient_UniqueOtherPhysicians\")\n",
    ")\n",
    "\n",
    "print(\"Aggregating beneficiary demographics...\")\n",
    "df_all_claims_bene_link = df_inpatient_train.select(\"ProviderID\", \"BeneID\").unionAll(\n",
    "    df_outpatient_train.select(\"ProviderID\", \"BeneID\")\n",
    ").distinct()\n",
    "\n",
    "df_bene_agg = df_all_claims_bene_link.join(df_beneficiary_train, \"BeneID\", \"left\").groupBy(\"ProviderID\").agg(\n",
    "    F.countDistinct(\"BeneID\").alias(\"Total_UniquePatients\"),\n",
    "    avg(year(current_date()) - year(col(\"DOB_clean\"))).alias(\"Avg_Patient_Age\"),\n",
    "    F.mean(\"Gender\").alias(\"Avg_Bene_Gender\"),\n",
    "    F.mean(\"Race\").alias(\"Avg_Bene_Race\"),\n",
    "    F.mean(\"RenalDiseaseIndicator\").alias(\"Avg_Bene_RenalDiseaseIndicator\"),\n",
    "    F.mean(\"NoOfMonths_PartACov\").alias(\"Avg_Bene_PartACovMonths\"),\n",
    "    F.mean(\"NoOfMonths_PartBCov\").alias(\"Avg_Bene_PartBCovMonths\"),\n",
    "    *[F.avg(f\"ChronicCond_{cond}\").alias(f\"Avg_ChronicCond_{cond}\") for cond in [\n",
    "        \"Alzheimer\", \"Heartfailure\", \"KidneyDisease\", \"Cancer\", \"ObstrPulmonary\",\n",
    "        \"Depression\", \"Diabetes\", \"IschemicHeart\", \"Osteoporasis\", \"rheumatoidarthritis\"]]\n",
    ")\n",
    "\n",
    "print(\"Joining features into final dataset...\")\n",
    "df_provider_features_baseline = df_providers_train.select(\"ProviderID\", LABEL_COL) \\\n",
    "    .join(df_inpatient_agg, \"ProviderID\", \"left\") \\\n",
    "    .join(df_outpatient_agg, \"ProviderID\", \"left\") \\\n",
    "    .join(df_bene_agg, \"ProviderID\", \"left\")\n",
    "\n",
    "numerical_cols = [c for c in df_provider_features_baseline.columns if c not in [\"ProviderID\", LABEL_COL]]\n",
    "for c in numerical_cols:\n",
    "    df_provider_features_baseline = df_provider_features_baseline.withColumn(\n",
    "        c, F.when(F.col(c).isNull(), 0).otherwise(F.col(c))\n",
    "    )\n",
    "\n",
    "print(f\"Final feature count: {len(numerical_cols)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 5. Train Baseline Model with scikit-learn\n",
    "\n",
    "print(\"Training RandomForest with scikit-learn...\")\n",
    "pdf = df_provider_features_baseline.toPandas()\n",
    "\n",
    "X = pdf[numerical_cols]\n",
    "y = pdf[LABEL_COL]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# COMMAND ----------\n",
    "# 6. Log to MLflow\n",
    "\n",
    "print(\"Logging metrics to MLflow...\")\n",
    "with mlflow.start_run(run_name=\"Baseline_Tabular_Model_Sklearn\"):\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestClassifier (sklearn)\")\n",
    "    mlflow.log_param(\"features\", \"tabular_only\")\n",
    "    mlflow.log_param(\"num_trees\", 100)\n",
    "\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"auc_roc\", auc)\n",
    "\n",
    "    mlflow.sklearn.log_model(clf, \"baseline_fraud_model_sklearn\")\n",
    "    dbutils.fs.put(\"/tmp/baseline_precision.txt\", str(precision), True)\n",
    "\n",
    "print(\"Baseline experiment logged successfully.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 7. Save Features to Delta Table\n",
    "\n",
    "print(\"Saving baseline features to Delta table for next notebook...\")\n",
    "df_provider_features_baseline.write.mode(\"overwrite\").saveAsTable(\"workspace.default.temp_provider_features_for_augmented_run\")\n",
    "\n",
    "print(\"\\u2705 Baseline features saved.\")\n",
    "print(\"\\ud83c\\udfd1 Notebook complete. Proceed to the graph-based experiment in Notebook 2.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fraud_Detection_Baseline.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}