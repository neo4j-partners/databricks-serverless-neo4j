# Databricks notebook source
catalog = "workspace"
schema = "default"
volume_base_path = f"/Volumes/{catalog}/{schema}/medical-instance-claims-data/"

# Set the current catalog and schema for the Spark session
spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE {schema}")

print(f"Current catalog: {spark.catalog.currentCatalog()}")
print(f"Current database: {spark.catalog.currentDatabase()}")
print(f"Base path for CSVs: {volume_base_path}")

# COMMAND ----------
# DBTITLE 1,0. Drop Existing Tables and Constraints (for idempotency)
# This section ensures that if the notebook is run multiple times,
# it will clean up previous creations to avoid "constraint already exists" errors.

print("Attempting to drop existing tables and their associated constraints...")

# Drop tables in an order that respects foreign key dependencies
# (e.g., tables with FKs dropped before tables they reference)
tables_to_drop_in_order = [
    "Outpatient_Claim", "Inpatient_Claim", "Beneficiary", "Provider",
    "Outpatient_Claim_Train", "Inpatient_Claim_Train", "Beneficiary_Train", "Provider_Train"
]

for table_name in tables_to_drop_in_order:
    print(f"Dropping table '{table_name}' if it exists...")
    spark.sql(f"DROP TABLE IF EXISTS {table_name}")

print("Finished attempting to drop existing tables.")

# COMMAND ----------
# DBTITLE 1,1. Create 'Provider' Table (Test Data)
provider_csv_path = f"{volume_base_path}Test-Provider.csv"
table_name_provider = "Provider"

print(f"Processing Provider data from: {provider_csv_path}")

# 1. Read the CSV into a DataFrame
df_provider = spark.read.csv(
    provider_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_provider.write.format("delta").mode("overwrite").saveAsTable(table_name_provider)
print(f"Delta table '{table_name_provider}' created from CSV.")

# 3. Add NOT NULL constraint to the primary key column (now 'ProviderID')
spark.sql(f"ALTER TABLE {table_name_provider} ALTER COLUMN ProviderID SET NOT NULL")
print(f"NOT NULL constraint added to {table_name_provider}.ProviderID.")

# 4. Add the Primary Key constraint
spark.sql(f"ALTER TABLE {table_name_provider} ADD CONSTRAINT pk_provider PRIMARY KEY (ProviderID) NOT ENFORCED RELY")
print(f"Primary Key constraint added to {table_name_provider}.")

# Verify the table schema and constraints
spark.sql(f"DESCRIBE EXTENDED {table_name_provider}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_provider} LIMIT 5").show()

# COMMAND ----------
# DBTITLE 1,2. Create 'Beneficiary' Table (Test Data)
beneficiary_csv_path = f"{volume_base_path}Test_Beneficiary.csv"
table_name_beneficiary = "Beneficiary"

print(f"\nProcessing Beneficiary data from: {beneficiary_csv_path}")

# 1. Read the CSV into a DataFrame
df_beneficiary = spark.read.csv(
    beneficiary_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_beneficiary.write.format("delta").mode("overwrite").saveAsTable(table_name_beneficiary)
print(f"Delta table '{table_name_beneficiary}' created from CSV.")

# 3. Add NOT NULL constraint to the primary key column
spark.sql(f"ALTER TABLE {table_name_beneficiary} ALTER COLUMN BeneID SET NOT NULL")
print(f"NOT NULL constraint added to {table_name_beneficiary}.BeneID.")

# 4. Add the Primary Key constraint
spark.sql(f"ALTER TABLE {table_name_beneficiary} ADD CONSTRAINT pk_beneficiary PRIMARY KEY (BeneID) NOT ENFORCED RELY")
print(f"Primary Key constraint added to {table_name_beneficiary}.")

# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_beneficiary}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_beneficiary} LIMIT 5").show()

# COMMAND ----------
# DBTITLE 1,3. Create 'Inpatient_Claim' Table (Test Data)
inpatient_csv_path = f"{volume_base_path}Test_Inpatient_Claim.csv"
table_name_inpatient_claim = "Inpatient_Claim" # Renamed table

print(f"\nProcessing Inpatient data from: {inpatient_csv_path}")

# 1. Read the CSV into a DataFrame
df_inpatient_claim = spark.read.csv(
    inpatient_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_inpatient_claim.write.format("delta").mode("overwrite").saveAsTable(table_name_inpatient_claim)
print(f"Delta table '{table_name_inpatient_claim}' created from CSV.")

# 3. Add NOT NULL constraints for PK and FK columns
spark.sql(f"ALTER TABLE {table_name_inpatient_claim} ALTER COLUMN ClaimID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_inpatient_claim} ALTER COLUMN BeneID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_inpatient_claim} ALTER COLUMN ProviderID SET NOT NULL")
print(f"NOT NULL constraints added to {table_name_inpatient_claim}.ClaimID, BeneID, ProviderID.")


# 4. Add Primary Key and Foreign Keys
spark.sql(f"ALTER TABLE {table_name_inpatient_claim} ADD CONSTRAINT pk_inpatient_claim PRIMARY KEY (ClaimID) NOT ENFORCED RELY")

# Foreign Key to Beneficiary
spark.sql(f"""
    ALTER TABLE {table_name_inpatient_claim} ADD CONSTRAINT fk_inpatient_claim_beneficiary
    FOREIGN KEY (BeneID) REFERENCES {table_name_beneficiary} (BeneID) NOT ENFORCED RELY
""")

# Foreign Key to Provider
spark.sql(f"""
    ALTER TABLE {table_name_inpatient_claim} ADD CONSTRAINT fk_inpatient_claim_provider
    FOREIGN KEY (ProviderID) REFERENCES {table_name_provider} (ProviderID) NOT ENFORCED RELY
""")
print(f"Primary and Foreign Key constraints added to {table_name_inpatient_claim}.")


# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_inpatient_claim}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_inpatient_claim} LIMIT 5").show()


# COMMAND ----------
# DBTITLE 1,4. Create 'Outpatient_Claim' Table (Test Data)
outpatient_csv_path = f"{volume_base_path}Test_Outpatient_Claim.csv"
table_name_outpatient_claim = "Outpatient_Claim" # Renamed table

print(f"\nProcessing Outpatient data from: {outpatient_csv_path}")

# 1. Read the CSV into a DataFrame
df_outpatient_claim = spark.read.csv(
    outpatient_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_outpatient_claim.write.format("delta").mode("overwrite").saveAsTable(table_name_outpatient_claim)
print(f"Delta table '{table_name_outpatient_claim}' created from CSV.")

# 3. Add NOT NULL constraints for PK and FK columns
spark.sql(f"ALTER TABLE {table_name_outpatient_claim} ALTER COLUMN ClaimID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_outpatient_claim} ALTER COLUMN BeneID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_outpatient_claim} ALTER COLUMN ProviderID SET NOT NULL")
print(f"NOT NULL constraints added to {table_name_outpatient_claim}.ClaimID, BeneID, ProviderID.")


# 4. Add Primary Key and Foreign Keys
spark.sql(f"ALTER TABLE {table_name_outpatient_claim} ADD CONSTRAINT pk_outpatient_claim PRIMARY KEY (ClaimID) NOT ENFORCED RELY")

# Foreign Key to Beneficiary
spark.sql(f"""
    ALTER TABLE {table_name_outpatient_claim} ADD CONSTRAINT fk_outpatient_claim_beneficiary
    FOREIGN KEY (BeneID) REFERENCES {table_name_beneficiary} (BeneID) NOT ENFORCED RELY
""")

# Foreign Key to Provider
spark.sql(f"""
    ALTER TABLE {table_name_outpatient_claim} ADD CONSTRAINT fk_outpatient_claim_provider
    FOREIGN KEY (ProviderID) REFERENCES {table_name_provider} (ProviderID) NOT ENFORCED RELY
""")
print(f"Primary and Foreign Key constraints added to {table_name_outpatient_claim}.")

# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_outpatient_claim}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_outpatient_claim} LIMIT 5").show()


# COMMAND ----------
# DBTITLE 1,5. Create 'Provider_Train' Table (Train Data)
provider_train_csv_path = f"{volume_base_path}Train-Provider.csv"
table_name_provider_train = "Provider_Train"

print(f"\nProcessing Provider Train data from: {provider_train_csv_path}")

# 1. Read the CSV into a DataFrame
df_provider_train = spark.read.csv(
    provider_train_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_provider_train.write.format("delta").mode("overwrite").saveAsTable(table_name_provider_train)
print(f"Delta table '{table_name_provider_train}' created from CSV.")

# 3. Add NOT NULL constraint to the primary key column
spark.sql(f"ALTER TABLE {table_name_provider_train} ALTER COLUMN ProviderID SET NOT NULL")
print(f"NOT NULL constraint added to {table_name_provider_train}.ProviderID.")

# 4. Add the Primary Key constraint
spark.sql(f"ALTER TABLE {table_name_provider_train} ADD CONSTRAINT pk_provider_train PRIMARY KEY (ProviderID) NOT ENFORCED RELY")
print(f"Primary Key constraint added to {table_name_provider_train}.")

# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_provider_train}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_provider_train} LIMIT 5").show()

# COMMAND ----------
# DBTITLE 1,6. Create 'Beneficiary_Train' Table (Train Data)
beneficiary_train_csv_path = f"{volume_base_path}Train_Beneficiary.csv"
table_name_beneficiary_train = "Beneficiary_Train"

print(f"\nProcessing Beneficiary Train data from: {beneficiary_train_csv_path}")

# 1. Read the CSV into a DataFrame
df_beneficiary_train = spark.read.csv(
    beneficiary_train_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_beneficiary_train.write.format("delta").mode("overwrite").saveAsTable(table_name_beneficiary_train)
print(f"Delta table '{table_name_beneficiary_train}' created from CSV.")

# 3. Add NOT NULL constraint to the primary key column
spark.sql(f"ALTER TABLE {table_name_beneficiary_train} ALTER COLUMN BeneID SET NOT NULL")
print(f"NOT NULL constraint added to {table_name_beneficiary_train}.BeneID.")

# 4. Add the Primary Key constraint
spark.sql(f"ALTER TABLE {table_name_beneficiary_train} ADD CONSTRAINT pk_beneficiary_train PRIMARY KEY (BeneID) NOT ENFORCED RELY")
print(f"Primary Key constraint added to {table_name_beneficiary_train}.")

# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_beneficiary_train}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_beneficiary_train} LIMIT 5").show()

# COMMAND ----------
# DBTITLE 1,7. Create 'Inpatient_Claim_Train' Table (Train Data)
inpatient_train_csv_path = f"{volume_base_path}Train_Inpatient_Claim.csv"
table_name_inpatient_claim_train = "Inpatient_Claim_Train" # Renamed table

print(f"\nProcessing Inpatient Train data from: {inpatient_train_csv_path}")

# 1. Read the CSV into a DataFrame
df_inpatient_claim_train = spark.read.csv(
    inpatient_train_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_inpatient_claim_train.write.format("delta").mode("overwrite").saveAsTable(table_name_inpatient_claim_train)
print(f"Delta table '{table_name_inpatient_claim_train}' created from CSV.")

# 3. Add NOT NULL constraints for PK and FK columns
spark.sql(f"ALTER TABLE {table_name_inpatient_claim_train} ALTER COLUMN ClaimID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_inpatient_claim_train} ALTER COLUMN BeneID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_inpatient_claim_train} ALTER COLUMN ProviderID SET NOT NULL")
print(f"NOT NULL constraints added to {table_name_inpatient_claim_train}.ClaimID, BeneID, ProviderID.")


# 4. Add Primary Key and Foreign Keys
spark.sql(f"ALTER TABLE {table_name_inpatient_claim_train} ADD CONSTRAINT pk_inpatient_claim_train PRIMARY KEY (ClaimID) NOT ENFORCED RELY")

# Foreign Key to Beneficiary_Train
spark.sql(f"""
    ALTER TABLE {table_name_inpatient_claim_train} ADD CONSTRAINT fk_inpatient_claim_train_beneficiary
    FOREIGN KEY (BeneID) REFERENCES {table_name_beneficiary_train} (BeneID) NOT ENFORCED RELY
""")

# Foreign Key to Provider_Train
spark.sql(f"""
    ALTER TABLE {table_name_inpatient_claim_train} ADD CONSTRAINT fk_inpatient_claim_train_provider
    FOREIGN KEY (ProviderID) REFERENCES {table_name_provider_train} (ProviderID) NOT ENFORCED RELY
""")
print(f"Primary and Foreign Key constraints added to {table_name_inpatient_claim_train}.")


# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_inpatient_claim_train}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_inpatient_claim_train} LIMIT 5").show()

# COMMAND ----------
# DBTITLE 1,8. Create 'Outpatient_Claim_Train' Table (Train Data)
outpatient_train_csv_path = f"{volume_base_path}Train_Outpatient_Claim.csv"
table_name_outpatient_claim_train = "Outpatient_Claim_Train" # Renamed table

print(f"\nProcessing Outpatient Train data from: {outpatient_train_csv_path}")

# 1. Read the CSV into a DataFrame
df_outpatient_claim_train = spark.read.csv(
    outpatient_train_csv_path,
    header=True,
    inferSchema=True
)

# 2. Write the DataFrame to a Delta Lake table
df_outpatient_claim_train.write.format("delta").mode("overwrite").saveAsTable(table_name_outpatient_claim_train)
print(f"Delta table '{table_name_outpatient_claim_train}' created from CSV.")

# 3. Add NOT NULL constraints for PK and FK columns
spark.sql(f"ALTER TABLE {table_name_outpatient_claim_train} ALTER COLUMN ClaimID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_outpatient_claim_train} ALTER COLUMN BeneID SET NOT NULL")
spark.sql(f"ALTER TABLE {table_name_outpatient_claim_train} ALTER COLUMN ProviderID SET NOT NULL")
print(f"NOT NULL constraints added to {table_name_outpatient_claim_train}.ClaimID, BeneID, ProviderID.")


# 4. Add Primary Key and Foreign Keys
spark.sql(f"ALTER TABLE {table_name_outpatient_claim_train} ADD CONSTRAINT pk_outpatient_claim_train PRIMARY KEY (ClaimID) NOT ENFORCED RELY")

# Foreign Key to Beneficiary_Train
spark.sql(f"""
    ALTER TABLE {table_name_outpatient_claim_train} ADD CONSTRAINT fk_outpatient_claim_train_beneficiary
    FOREIGN KEY (BeneID) REFERENCES {table_name_beneficiary_train} (BeneID) NOT ENFORCED RELY
""")

# Foreign Key to Provider_Train
spark.sql(f"""
    ALTER TABLE {table_name_outpatient_claim_train} ADD CONSTRAINT fk_outpatient_claim_train_provider
    FOREIGN KEY (ProviderID) REFERENCES {table_name_provider_train} (ProviderID) NOT ENFORCED RELY
""")
print(f"Primary and Foreign Key constraints added to {table_name_outpatient_claim_train}.")

# Verify
spark.sql(f"DESCRIBE EXTENDED {table_name_outpatient_claim_train}").show(truncate=False)
spark.sql(f"SELECT * FROM {table_name_outpatient_claim_train} LIMIT 5").show()

print("\nAll tables (Test and Train) created and constraints declared successfully (assuming permissions are correct)!")
